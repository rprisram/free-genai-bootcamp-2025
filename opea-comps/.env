# Port for LLM endpoint (default is 8008)
LLM_ENDPOINT_PORT=9000

# Model ID for Ollama (e.g., llama2, mistral, etc.)
LLM_MODEL_ID=llama3.2:1b

# Host IP (usually localhost or your machine's IP)
host_ip=localhost

# Proxy settings (leave empty if not using proxies)
http_proxy=
https_proxy=
no_proxy= 
TELEMETRY_ENDPOINT=http://localhost:4318/v1/traces